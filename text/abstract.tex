
The usual way to do research in compilers, moreover in Feedback Directed Optimization is to construct a framework and devise an experiment based on single-run input training and single data testing. Recently some researchers have argued about the reliability of such experiments, and developed other approaches to this problem. Usually using repetition of experiments and collecting data to perform a reliable statistical analysis. This paper also discusses these issues and aims to construct an experiment to show a false speedup from actual data. This was done by just ignoring the multiple run strategy and literally selecting parts of the collected data to show that, in a single-run scheme, it can happen. As conclusion the paper states that the only way to avoid these problems is to define and use a reliable methodology based on solid statistical measurements. In this paper the methodology called {\em combined profiling} (\CP) is also presented, and it is shown that employing it can generate more reliable results. \FDI\ decisions are shown to be more accurate using \CP\ instead of single-run evaluation.
