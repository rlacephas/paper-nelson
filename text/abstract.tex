
The usual way to do research in compilers, moreover in Feedback Directed Optimization is to construct a framework and devise an experiment based on single-run input training and single data testing. Recently some researchers have argued about the reliability of such experiments, and developed other approaches to this problem. Usually using repetition of experiments and collecting data to perform a reliable statistical analysis. This paper also discusses these issues and we aimed to construct an experiment to show a false speedup from actual data. We did this by just ignoring our multiple runs strategy and literally picking parts of our collected data to show that it could happen in a single-run scheme. In conclusion we state that the only way to avoid these problems is to define and use a reliable methodology based on solid statistical measurements. In this paper we also present our methodology called {\em combined profiling} (\CP), and show that using it we can have reliable results. We showed that \FDI\ decisions can be more accurate using \CP\ instead of single-run evaluation.
