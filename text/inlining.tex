
Function inlining, or simply inlining, is a classic code transformation that can significantly increase the performance of many programs.  A compiler pass that decides which calls to inline, and in which order, is referred to as an inliner.  The basic idea of inlining is straightforward: rather than making a function call, replace the call in the originating function with a copy of the body of the function to be called.  Berube  describes the existing inliner in \llvm, and then presents a new feedback-directed inliner (\FDI) that uses \CP~\cite{BerubePhD}. The \FDI\ inlining strategies proposed by Berube and the \llvm\ inliner will are used in this paper to illustrate the need for care when attempting to predict the performance of an \FDO\  transformation with a benchmark-based performance study.
All inliners discussed in this paper are implemented in the open-source \llvm\ compiler~\cite{LattnerAdveCGO04}.

Some terminology is required to discuss the inlining process.  The function making a call is referred to as the {\it caller}, while the called function is the {\it callee}.  The representation of a call in a compiler's {\it internal representation} (IR) is a {\it call site}. In \llvm, a call site is an instruction that indicates both the caller and the callee.  Inlining inserts a copy of the callee at a call site. Casually practicioners speak of "inlining a function" which is imprecise. What gets inlined is a call site, the function itself may remain in the code as it may still be invoked from several other call sites. A function can only be safely removed from the code base after all its call sites have been inlined. A callee of an inlined call site may itself contain call sites, which are copied into the caller to produce new call sites.  The call site where inlining occurs is called the {\it source} call site.  A call site in the callee that is copied during inlining is called an {\it original} call site, and the new copy of the original call site inside the caller is called the {\it target} call site.

\subsection{Barriers to Inlining}

Not every call site can be inlined.  Indirect calls use a pointer variable to identify the location of the called code, and arise from function pointers and dynamically-polymorphic call dispatching.  Such calls cannot be inlined because the callee is unknown at compiler time.  External calls into code not currently available in the compiler, such as calls into different modules or to statically-linked library functions, cannot be inlined before link time because the source representation of the callee is not available in the compiler. Calls to dynamically-linked libraries can never be inlined by definition. Moreover, call sites whose callee uses a \name{setjump} instruction cannot be inlined because a \name{setjump} can redirect program control flow {\it anywhere}, including to the middle of different function, without using the call/return mechanisms.  Inlining the \name{setjump} could cause manual stack management at the target of the jump to be incorrect leading the inlined version to not be functionally equivalent to the original code.

\subsection{Benefits of Inlining}

Inlining a call site has a small direct benefit.  Removing the call reduces the number of executed instructions because the {\tt call} instruction in the caller becomes unnecessary, as does the {\tt return} instruction in the callee.  Furthermore, any parameters passed to the callee and any values returned no longer need to be pushed onto the stack\footnote{Some calling conventions allow values to pass between  the caller and callee in registers.}.

However, the greatest potential benefit of inlining comes from additional code simplification that it may enable by bringing the callee's code into the caller's scope~\cite{BerubePhD}. Many code analysis algorithms work within the scope of a single function; inter-procedural analysis is usually fundamentally more difficult, and always computationally more expensive than intra-procedural analysis, because of the increased scope.  A function call limits the scope of analyses and is a barrier to code motion because the caller sees the callee as a ``black box'' with unknown effect.

\subsection{Costs of Inlining}

Inlining non-profitable call sites can indirectly produce negative effects.  The increased scope provided for analysis by inlining also increases the costs of these analyses.  Most algorithms used by compilers have super-linear time complexity.  Extremely large procedures may take excessively long to analyze; some compilers will abort an analysis that takes too long.  Furthermore, a program must be loaded into memory from disk before it can be executed.  A larger executable file size increases a program's start-up time.  Finally, developers eschew unnecessarily large program binaries because of the costs associated with the storage and transmission of large files for both the developer and their clients. Therefore, inlining that does not improve performance should be avoided.

\subsection{Inlining-Invariant Program Characteristics}

While inlining a call site causes a large change in the caller's code, it has a minimal direct impact o the use of memory system resources at run time~\cite{BerubePhD}.  Ignoring the subsequent simplifications the inlining enables, inlining proper has no appreciable impact on register use, data cache or instruction cache efficiency.  Regardless of inlining, the same dynamic sequence of instructions must process the same data in the same order to produce the same deterministic program result.

Inlining should have negligible impact on register spills. The additional variables introduced into the caller by inlining place additional demands on the register allocator, and may increase the number of register spills introduced into the caller.  However, without inlining, the calling convention requires the caller to save any live registers before making a call, or for the callee to save any registers before it uses them; in both cases, these registers must be restored before resuming execution in the caller. Thus, inlining merely shifts the responsibility for register management from the calling convention to the register allocator.

Similarly, inlining does not change the data memory accesses of a program.  Whether in the caller or the callee, the same loads and stores, in the same order, are required for correct computation. Subsequent transformations may reorder independent memory accesses to better hide cache latency, or eliminate unnecessary accesses altogether, but this is not a direct consequence of inlining.  Thus, data cache accesses do not change with inlining, and nor does the cache miss rate.  

\subsection{Feedback-Directed Inlining (FDI)}

A commonly held belief amongst compiler designers is that inlining decisions should be sensitive to the frequency of execution of control-flow paths in a program. The premise is that with limited budget a compiler should select the most profitable call sites to inline and that the most profitable call sites will be the ones that execute most frequently. The limited budget arises from a desire to limit code growth in order to prevent the scope on non-linear-time static analysis from reaching sizes that would make such analyses impractical. The most common technique still used in compiler research and practice is to estimate the execution frequency of alternate control-flow paths from a single {\em profiling run} using a single input for a given program. Combined Profiling (CP) is a methology that allows this prediction to use information collected from multiple executions of a program~\cite{BerubeISPASS}. Berube developed a new CP-driven Feedback-Directed Inliner that is a worklist algorithm whose decisions are based on tuneable cost/benefit functions~\cite{BerubePhD}. This inliner is used in the remainder of this paper to study the performance evaluation methodology that is necessary for an accurate assessment of the performance implications of an FDO-based code transformation. Along the way the discussion points out the missleading conclusions that could be drawn from the collected data if the methodology is overlooked.
